'''
Long read assembly and post-processing workflow for assembling genomes from isolates and metagenomes.
Authors: Eli Moss, Benjamin Siranosian, Dylan Maghini
'''

import os
import glob
from pathlib import Path
import git
import snakemake

localrules: no_merge, basecall_staging, pilon_ranges, pilon_aggregate_vcf, assemble_final, faidx, extract_bigtigs, circularize_final, polish_final

# new sample input file has three columns, third is optional and only
# necessary if doing short read polishing
# SAMPLE    FAST5/FQ    SHORT_READS
fq_files_dict = {}
fast5_files_dict = {}
fast5_basename_to_path = {}
sr_polish_dict = {}
with open(config['file_names_txt'],'r') as f:

    for line in f:
        items = line.strip().split("\t")
        if len(items) == 1 or items[0] == 'Sample' or items[0] == '#Sample' or items[0].startswith('#'):
            continue
        sample = items[0]
        if not os.path.exists(sample):
            os.makedirs(sample)
        # if this sample is already basecalled
        if items[1].endswith(".fq.gz") or items[1].endswith(".fq") or items[1].endswith(".fastq.gz") or items[1].endswith(".fastq"):
            fq_files_dict[sample] = items[1]
            fast5_files_dict[sample] = ""

        # Otherwise we assume its a fast5 directory
        else:
            # find fast5 files. These are expected to be below the directory provided in the config.
            # All fast5's below this directory will be processed.
            # list provided directory recursively
            fast5_files_dict[sample] = glob.glob(os.path.join(items[1], '**', '*.fast5'), recursive=True)
            fq_files_dict[sample] = os.path.join(sample, "0.basecall", sample + ".fq")
            #create a dictionary which relates fast5 basenames to the full path of each file
            for f in fast5_files_dict[sample]:
                fast5_basename_to_path[os.path.splitext(os.path.basename(f))[0]] = f

        # if items is length 3, then we have a SR dataset as well
        if len(items)==3:
            split_reads = items[2].split(',')
            # don't think I need to check this as single end should also be supported
            # if len(split_reads) != 2:
                # sys.exit("Short reads specifiied must be length 2 and separated by a comma. You gave: " + items[2])
            sr_polish_dict[sample] = split_reads
        else:
            sr_polish_dict[sample] = ''


# print(fq_files_dict)
print(sr_polish_dict)

#set up singularity image used for the bulk of rules
singularity_image = "shub://elimoss/lathe:longread"

#extract samplename from config
SAMPLES = list(fq_files_dict.keys())

#perform a check on the Lathe git repo and warn if not up to date
onstart:
    import git
    print("Checking for updates or modifications to workflow")
    repo_dir = os.path.dirname(workflow.snakefile)
    repo = git.Repo(repo_dir)
    assert not repo.bare
    repo_git = repo.git
    stat = repo_git.diff('origin/master')
    if stat != "":
        print('WARNING: Differences to latest version detected. Please reset changes and/or pull repo.')
    else:
        print("No updates or modifications found")

rule all:
    input:
        expand("{sample}/2.polish/{sample}_polished.fasta", sample = SAMPLES), #request final output

#Basecalling and assembly
########################################################
rule polish_final:
    #Generate the final output of the polishing phase, whether that's with short reads, long reads, both or neither.
    #Colons and any following characters are omitted from contig names.
    input: config['assembly']
    output: "{sample}/2.polish/{sample}_polished.fasta"
    params:
		asm = config['assembly']
    shell: """
        cp {params.asm} {sample}/2.polish/{sample}_polished.fasta
        """

#Circularization
#########################################################
checkpoint extract_bigtigs:
    #Only genome-scale contigs are tested for circularity. This rule defines what size that is and extracts those contigs
    #to individual fasta files.
    input:
        rules.polish_final.output,
        rules.polish_final.output[0] + ".fai",
    output: directory("{sample}/3.circularization/1.candidate_genomes/")
    singularity: singularity_image
    params:
        min_size = 800000,
        sample = "{sample}"
    shell: """
        mkdir -p {sample}/3.circularization
        mkdir -p {output}
        cat {input[1]} | awk '{{if ($2 > {params.min_size}) print $1}}' | xargs -n 1 -I foo sh -c "
            /share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools faidx {input[0]} foo -o {params.sample}/3.circularization/1.candidate_genomes/foo.fa
        "

        cat {input[1]} | awk '{{if ($2 > {params.min_size}) print $1}}' | xargs -n 1 -I foo sh -c "
            /share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools faidx {input[0]} foo -o {params.sample}/3.circularization/1.candidate_genomesfoo.fa
        "
        """

rule circularize_bam2reads:
    #Extracts reads mapping to the genome-scale contigs to be tested for circularity. These reads are reformatted to fastq and compressed.
    input:
        rules.polish_final.output[0] + '.bam',
        "{sample}/3.circularization/1.candidate_genomes/{tig}.fa",
        rules.polish_final.output[0] + '.bam.bai',
    output:
        "{sample}/3.circularization/2.circularization/spanning_tig_circularization/{tig}/{tig}_terminal_reads.fq.gz"
    params:
        tig = "{tig}"
    singularity: singularity_image
    shell: """
        (/share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools idxstats {input[0]} | grep {params.tig} | awk '{{if ($2 > 50000) print $1, ":", $2-50000, "-", $2; else print $1, ":", 1, "-", $2 }}' | tr -d ' ';
         /share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools idxstats {input[0]} | grep {params.tig} | awk '{{if ($2 > 50000) print $1, ":", 1, "-", 50000; else print $1, ":", 1, "-", $2 }}' | tr -d ' ') |
        xargs -I foo sh -c '/share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools view -h {input[0]} foo | /share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools fastq - || true' | paste - - - - | sort | uniq | tr '\t' '\n' | bgzip > {output}
        """

rule circularize_assemble:
    #Reads extracted in circularize_bam2reads are assembled with Canu in order to recover a contig spanning the two
    #ends of a circular genome contig
    input:
        rules.circularize_bam2reads.output
    output: "{sample}/3.circularization/2.circularization/spanning_tig_circularization/{tig}/assembly.fasta"
    params:
        directory="{sample}/3.circularization/2.circularization/spanning_tig_circularization/{tig}",
    singularity: "docker://quay.io/biocontainers/flye:2.4.2--py27he860b03_0"
    resources:
        time=4,
        mem=32
    threads: 4
    shell: """
        /share/inspurStorage/home1/jialh/human_virome/tools/Flye/bin/flye -t {threads} --nano-raw {input} -o {params.directory} -g 1m
        """
        #needed for canu:
        #canu -useGrid=False -assemble -p {wildcards.tig} -d {params.directory}  \
        #-nanopore-corrected {input} genomeSize=100000
        #{tig}.contigs
        #singularity_image #

rule circularize_spantig_pre:
    #Prepare to determine if the contig assembled in circularize_assemble actually spans the two ends of the putative genome contig.
    #Preparation entails performing an alignment of the potentially spanning contig to the putative genome contig and post-processing
    #the result.
    input:
        "{sample}/3.circularization/1.candidate_genomes/{tig}.fa", #rules.extract_bigtigs.output
        rules.circularize_assemble.output,
        rules.circularize_assemble.output[0] + '.fai'
    output:
        "{sample}/3.circularization/2.circularization/spanning_tig_circularization/{tig}/potential_circularization_alignments.tsv"
    singularity: singularity_image
    params:
        directory = "{sample}/3.circularization/2.circularization/spanning_tig_circularization/{tig}",
        prefix="spanning_tigs_to_ref"
    threads: 4
    resources:
        time=4,
        mem=16
    shell: """
        nucmer -b 5000 {input[0]} {input[1]} -p {params.directory}/{params.prefix} #-t {threads} #reverted nucmer back down to 3, no more multithreading :(

        delta-filter -q {params.directory}/{params.prefix}.delta > {params.directory}/{params.prefix}.filt.delta

        show-coords -Tq {params.directory}/{params.prefix}.filt.delta | cut -f8,9 | sed '1,3d' | sort | \
        uniq -c | tr -s ' ' '\\t' | cut -f2-99 | grep -v ^1 | cut -f2,3 > {params.directory}/potential_circularizations.tsv || true

        show-coords -Tql {params.directory}/{params.prefix}.filt.delta | grep -f {params.directory}/potential_circularizations.tsv | cat > {output} || true
        """

rule circularize_spantig:
    #Run the script which determines if the putative genome contig is spanned by the smaller contig assembled from terminal reads,
    #indicating circularity.
    input: rules.circularize_spantig_pre.output
    output: "{sample}/3.circularization/2.circularization/spanning_tig_circularization/{tig}/contig_spanned.txt"
    params:
        margin=10000
    script:
        "/share/inspurStorage/home3/ZXMGroup/VIROME/G3compare/pipeline/lathe/scripts/spancircle.py"

rule circularize_span_trim:
    #Trim circularized genome contigs at the determined wrap-around point
    input:
        rules.circularize_spantig.output,
        rules.extract_bigtigs.output[0] + '{tig}.fa',
        rules.extract_bigtigs.output[0] + '{tig}.fa.fai',
        rules.circularize_assemble.output,
        rules.circularize_assemble.output[0] + '.fai'
    output:
        "{sample}/3.circularization/3.circular_sequences/sh/{tig}_span_trim.sh"
    params:
        outfa = "{sample}/3.circularization/3.circular_sequences/{tig}_spanned.fa"
    run:
        span_out = open(input[0], 'r').readlines()
        cmd = ''
        if span_out == ['done\n'] or span_out[0].strip() == 'no circularizations': #no circularization occurred
            print('Nothing to do')
        else:
            trim = span_out[0].strip()
            trim_cmd = '/share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools faidx ' + input[1] + ' ' + trim + " > " + params.outfa + "\n"
            cmd += trim_cmd

            if len(span_out) == 3:
                extend = span_out[1].strip()
                extend_cmd = '/share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools faidx ' + input[3] + ' ' + extend + " | grep -v '>'" + " >> " + params.outfa + "\n"
    #           print(extend_cmd)
                cmd += extend_cmd

        open(output[0], 'w').write(cmd + '\n')

def aggregate_span_trim(wildcards):
    #Collect the genome sequences produced by spanning contig circularization
    checkpoint_output = checkpoints.extract_bigtigs.get(**wildcards).output[0]
    result = expand(rules.circularize_span_trim.output, #"{sample}/3.circularization/3.circular_sequences/sh/{tig}_span_trim.sh",
        sample=wildcards.sample,
        tig=glob_wildcards(os.path.join(checkpoint_output, '{tig}.fa')).tig)
    return(result)

rule circularize_overcirc:
    #Test putative genome contigs for circularity by self-alignment and determination of 'overcircularization',
    #assembly beyond the circular wraparound point. This produces redundant sequences at the termini of the putative
    #genome contig which can be determined by corner-cutting off-diagonal alignments in a self-alignment dotplot.
    input:
        "{sample}/3.circularization/1.candidate_genomes/{tig}.fa"
    output: "{sample}/3.circularization/2.circularization/overcircularized/overcirc_{tig}.txt"
    params:
        delta = '{sample}/3.circularization/2.circularization/overcircularized/{tig}'
    threads: 8
    singularity: singularity_image
    script:
        "/share/inspurStorage/home3/ZXMGroup/VIROME/G3compare/pipeline/lathe/scripts/encircle.py"

rule circularize_overcirc_trim:
    #Trim the genome contig circularized by overcircularization detection according to the determined wraparound point.
    input:
        rules.circularize_overcirc.output,
        rules.extract_bigtigs.output[0] + "{tig}.fa",
        rules.extract_bigtigs.output[0] + '{tig}.fa.fai',
        rules.circularize_assemble.output,
        rules.circularize_assemble.output[0] + '.fai'
    output:
        "{sample}/3.circularization/3.circular_sequences/sh/{tig}_span_overcirc.sh"
    params:
        outfa = "{sample}/3.circularization/3.circular_sequences/{tig}_overcirc.fa"
    run:
        span_out = open(input[0], 'r').readlines()
        cmd = ''
        if span_out == ['done\n']: #no circularization occurred
            print('No over-circularization')
        else:
            trim = span_out[0].strip()
            trim_cmd = '/share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools faidx ' + input[1] + ' ' + trim + " > " + params.outfa + "\n"
            cmd += trim_cmd

        open(output[0], 'w').write(cmd + '\n')

def aggregate_overcirc_trim(wildcards):
    #Collect the circular genome sequences obtained by overcircularization detection.
    checkpoint_output = checkpoints.extract_bigtigs.get(**wildcards).output[0]
    result = expand(rules.circularize_overcirc_trim.output, #"{sample}/3.circularization/3.circular_sequences/sh/{tig}_span_overcirc.sh",
        sample=wildcards.sample,
        tig=glob_wildcards(os.path.join(checkpoint_output, '{tig}.fa')).tig)
    return(result)

rule circularize_final:
    #Collect the circular genome sequences and add them back into the total assembly fasta files
    #This is done by generating a .sh file for each putative genome which either yields the circularized
    #sequence or no sequence, depending on whether circularization was successful.
    input:
        rules.polish_final.output,
        rules.polish_final.output[0] + '.fai',
        aggregate_overcirc_trim,
        aggregate_span_trim
    output:
        '{sample}/3.circularization/4.{sample}_circularized.fasta'
    threads: 1
    params:
        sample = "{sample}"
    singularity: singularity_image
    shell: """
        find {params.sample}/3.circularization/3.circular_sequences/sh/ -type f | xargs cat | bash
        ls {params.sample}/3.circularization/3.circular_sequences/ | grep .fa$ | cut -f1-2 -d '_' > circs.tmp || true
        (cat {input[1]} | grep -vf circs.tmp |
        cut -f1 | xargs /share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools faidx {input[0]}; ls {params.sample}/3.circularization/3.circular_sequences/* | grep .fa$ | xargs cat) |
        tr -d '\\n' | sed 's/\\(>[contigscaffold_]*[0-9]*\\)/\\n\\1\\n/g' | fold -w 120 | cut -f1 -d ':' | grep -v '^$' > {output} || true
        rm circs.tmp
        """

#Misassembly detection
#########################################################

def skip_circularization_or_not():
    #Allows all circularization to be skipped when unneeded
    if config['skip_circularization'] == 'True' or config['skip_circularization'] == True:
        return(rules.polish_final.output)
    else:
        return(rules.circularize_final.output)

rule final:
    #Perform one last round of misassembly detection and removal, followed by generation of the final output assembly file
    input: skip_circularization_or_not()[0].replace('.fasta', '.corrected.fasta') #perform one last round of misassembly breakage
    output: "{sample}/5.final/{sample}_final.fa"
    shell: "cp {input} {output}"

#Utility functions
#########################################################

rule align_paf:
    #Align long reads to a fasta, storing the result in .paf format.
    input:
        '{ref}.f{asta}',
        list(fq_files_dict.values())
    output:
        "{ref}.f{asta}.paf"
    threads: 8
    singularity: singularity_image
    shell:
        "minimap2 -t {threads} -x map-ont {input} > {output}"

print(fq_files_dict)
rule align_bam:
    #Align long reads to a fasta, storing the result in .bam format.
    input:
        '{ref}.f{asta}', #the asta bit makes this work for .fa and .fasta files
        list(fq_files_dict.values())
    output:
        "{ref}.f{asta}.bam"
    threads: 16
    resources:
        time=6,
        mem=16
    singularity: singularity_image
    shell:
        "minimap2 -t {threads} -ax map-ont {input} | /share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools sort --threads {threads} -o {output}"

rule bam_idx:
    #index a .bam file
    input:
        '{some}.bam'
    output:
        '{some}.bam.bai'
    singularity: singularity_image
    shell:
        "/share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools index {input}"

rule faidx:
    #index a .fasta file
    input: '{something}.f{asta}'
    output: '{something}.f{asta}.fai'
    singularity: singularity_image
    shell: """
    echo /share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools faidx {input}
    /share/inspurStorage/home1/jialh/tools/samtools-1.9/samtools faidx {input}
    """